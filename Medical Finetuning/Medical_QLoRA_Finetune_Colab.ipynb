{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5f712f8",
   "metadata": {},
   "source": [
    "# Medical Finetuning With QLoRA Using Unsloth in Colab\n",
    "This notebook demonstrates how to fine-tune a medical language model using QLoRA and Unsloth in Google Colab. We will use a domain-specific medical dataset, configure Unsloth for 4-bit quantized low-rank adaptation, and run the full training workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24647742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d218a96",
   "metadata": {},
   "source": [
    "## 1. Install Unsloth and Dependencies\n",
    "Install Unsloth and the required libraries for training and dataset handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85da275",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unsloth[torch] --upgrade\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bda790c",
   "metadata": {},
   "source": [
    "## 2. Load a Medical Dataset\n",
    "Load a domain-specific medical dataset. You can use a public dataset from HuggingFace or upload your own clinical Q&A pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f8cf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Example: Replace with your own dataset if needed\n",
    "# This uses a public medical QA dataset\n",
    "medical_dataset = load_dataset(\"medal/medical-qa\", split=\"train\")\n",
    "print(medical_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20d84b3",
   "metadata": {},
   "source": [
    "## 3. Load the Base Model with Unsloth\n",
    "Load a base model such as Llama 3 or DeepSeek-R1 in 4-bit mode using Unsloth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca11a248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",  # or \"unsloth/deepseek-llm-7b-bnb-4bit\"\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25d2e11",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for Training\n",
    "Format your dataset as instruction/response pairs for supervised fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2a5472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(example):\n",
    "    return {\n",
    "        \"instruction\": example[\"question\"],\n",
    "        \"output\": example[\"answer\"]\n",
    "    }\n",
    "\n",
    "train_data = [format_example(x) for x in medical_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52398e2f",
   "metadata": {},
   "source": [
    "## 5. Tokenize Data\n",
    "Tokenize the formatted data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e783414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import get_supervised_data_module\n",
    "\n",
    "data_module = get_supervised_data_module(\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_data,\n",
    "    max_seq_length = 2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4779c7",
   "metadata": {},
   "source": [
    "## 6. Configure QLoRA Training\n",
    "Set up the QLoRA training configuration, including epochs, batch size, and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d84721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastTrainer\n",
    "\n",
    "trainer = FastTrainer(\n",
    "    model = model,\n",
    "    train_dataset = data_module[\"train_dataset\"],\n",
    "    eval_dataset = None,\n",
    "    tokenizer = tokenizer,\n",
    "    epochs = 2,  # Adjust as needed\n",
    "    batch_size = 2,\n",
    "    gradient_accumulation_steps = 8,\n",
    "    lr = 2e-4,\n",
    "    save_steps = 100,\n",
    "    output_dir = \"qlora-medical-adapter\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce9f34b",
   "metadata": {},
   "source": [
    "## 7. Train the Model\n",
    "Start the training process. Monitor GPU memory and performance using Colab's resource panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6011d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8db4b5d",
   "metadata": {},
   "source": [
    "## 8. Save the Fine-Tuned Adapter\n",
    "Save the trained adapter for later use or deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfcb07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"qlora-medical-adapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f824b265",
   "metadata": {},
   "source": [
    "## 9. Test the Fine-Tuned Model\n",
    "Test the model's response to new medical queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8452738",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What are the symptoms of diabetes?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0392199",
   "metadata": {},
   "source": [
    "---\n",
    "**Tips:**\n",
    "- Adjust batch size and gradient accumulation for Colab memory limits.\n",
    "- Use `!nvidia-smi` in a code cell to monitor GPU memory.\n",
    "- For custom datasets, ensure your data is in instruction/output format."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
